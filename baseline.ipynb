{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [00:00, 151.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'A', u'DT', u'A'), (u'bus', u'NN', u'bus'), (u'looms', u'VBZ', u'looms'), (u'out', u'IN', u'out'), (u'of', u'IN', u'of'), (u'the', u'DT', u'the'), (u'grey', u'JJ', u'grey'), (u'and', u'CC', u'and'), (u'blizzardy', u'JJ', u'blizzardy'), (u'conditions', u'NNS', u'conditions'), (u'and', u'CC', u'and'), (u'we', u'PRP', u'we'), (u'get', u'VB', u'get'), (u'on', u'IN', u'on'), (u'gratefully', u'RB', u'gratefully'), (u'.', u'.', u'.')]\n",
      "500.0 <tp=0  tn=469  fp=0  fn=30  total=500  prec=1.00  rec=0.00>\n",
      "1000.0 <tp=0  tn=941  fp=1  fn=57  total=1000  prec=0.00  rec=0.00>\n",
      "1500.0 <tp=0  tn=1396  fp=1  fn=102  total=1500  prec=0.00  rec=0.00>\n",
      "2000.0 <tp=0  tn=1860  fp=1  fn=138  total=2000  prec=0.00  rec=0.00>\n",
      "2500.0 <tp=0  tn=2324  fp=1  fn=174  total=2500  prec=0.00  rec=0.00>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7b7e3a7dfaa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         similars = [(w, sim) for w, sim in index[error]\n\u001b[0m\u001b[0;32m    126\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfreq_dist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                     freq_dist[w] / freq_dist[error] >= 30]\n",
      "\u001b[1;32mC:\\Users\\RudnikC\\Documents\\Personnel\\LCSS\\Project\\spellchecker\\lib\\CharacterIndex.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RudnikC\\Documents\\Personnel\\LCSS\\Project\\spellchecker\\lib\\CharacterIndex.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, w, n)\u001b[0m\n\u001b[0;32m     56\u001b[0m             [(self.words[i], f)\n\u001b[0;32m     57\u001b[0m              \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m              if self.words[i] != w],\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RudnikC\\Documents\\Personnel\\LCSS\\Project\\spellchecker\\lib\\TermIndex.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, _id)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0m_id\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from config import CONFIG\n",
    "\n",
    "from tests import tests1, tests2\n",
    "\n",
    "from normalizer import Normalizer\n",
    "\n",
    "from lib.CharacterIndex import CharacterIndex\n",
    "from lib.NaiveTokenizer import NaiveTokenizer\n",
    "from lib.TextStreamer import TextStreamer\n",
    "from lib.CONLL14ErrorCorrection import CONLL14ErrorCorrection\n",
    "from lib.Parser import PatternParser\n",
    "from lib.Report import Report\n",
    "\n",
    "from lib.Tools import (\n",
    "    FreqDist,\n",
    "    splitter,\n",
    "    strip_punct,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict as deft\n",
    ")\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    return '.'.join([str(t) for t in time.localtime()[3:6]])\n",
    "\n",
    "\n",
    "def get_name(template):\n",
    "    i = 1\n",
    "    while True:\n",
    "        name = template % (timestamp(), i)\n",
    "        if not os.path.exists(name):\n",
    "            return name\n",
    "        i += 1\n",
    "\n",
    "PoS_l = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT',\n",
    "         'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n",
    "         'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "PoS = {}\n",
    "i = 1\n",
    "for k in PoS_l:\n",
    "\tPoS[k] = i\n",
    "\ti += 1\n",
    "\n",
    "raw_input()\n",
    "\n",
    "corpus = 'data/delorme.com_shu.pages_89.txt'\n",
    "\n",
    "report = Report()\n",
    "\n",
    "parser = PatternParser()\n",
    "\n",
    "for C in CONFIG:\n",
    "    \n",
    "#     tests = tests1.items() + tests2.items()\n",
    "\n",
    "    conll = CONLL14ErrorCorrection()\n",
    "    \n",
    "    tests = []\n",
    "    for (left, err, right, corr, category), human  in conll:\n",
    "        if err:\n",
    "            test = (left, strip_punct(err).lower(), right,\n",
    "                    strip_punct(corr).lower(), category, True)\n",
    "        else:\n",
    "            test = (left, strip_punct(corr), right, err, category, False)\n",
    "        tests.append(test)\n",
    "    tests = tests[:10000]\n",
    "\n",
    "    targets = [test[1] for test in tests]\n",
    "\n",
    "    #\tCollect input from large text file:\n",
    "    dump = []\n",
    "    train_conll=[]\n",
    "    for doc in TextStreamer(corpus, nb_sent=100):\n",
    "        for sent in splitter(doc):\n",
    "            parse = parser(sent)\n",
    "            for unit in parse.split():\n",
    "                input_sent = []\n",
    "                # print unit, '$ \\n'\n",
    "                for word_gr in unit :\n",
    "                    word = word_gr[0]\n",
    "                    tag = word_gr[1]\n",
    "                    _tuple = (word,tag,word)\n",
    "                    # print _tuple\n",
    "                    input_sent.append(_tuple)\n",
    "                # print input_sent    \n",
    "                # raw_input()\n",
    "            train_conll.append(input_sent)    \n",
    "            dump += [w.lower() for w in tokenizer(sent)]\n",
    "    freq_dist = Counter(dump + targets)\n",
    "    print train_conll[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\tMap all character n-grams to words, and all words to their\n",
    "    #\tcharacter n-grams\n",
    "    index = CharacterIndex(dump + targets, top_n=C['top_n'], min_r=C['sim_thres'])\n",
    "    index.build()\n",
    "\n",
    "    for left, error, right, correct, category, human in tests:\n",
    "        \n",
    "        if error == correct:\n",
    "            continue\n",
    "\n",
    "        report.add()\n",
    "\n",
    "        if human:\n",
    "            left = ' '.join(left.split()[-10:])\n",
    "            right = ' '.join(right.split()[:10])\n",
    "        else:\n",
    "            left = ''\n",
    "            right = ''\n",
    "\n",
    "        similars = [(w, sim) for w, sim in index[error]\n",
    "                    if freq_dist[w] >= 10 and\n",
    "                    freq_dist[w] / freq_dist[error] >= 30]\n",
    "\n",
    "        if not similars and not human:\n",
    "            report.tn(left, error, right, correct, category)\n",
    "            continue\n",
    "        elif not similars:\n",
    "            report.fn(left, error, right, correct, category)\n",
    "            continue\n",
    "        elif similars and not human:\n",
    "            report.fp(left, error, right, correct, category)\n",
    "            continue\n",
    "\n",
    "        similars.sort(\n",
    "            key=lambda x: freq_dist[x[0]],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        top = [w for w, _ in similars[:1]]\n",
    "        if correct in top:\n",
    "            report.tp(left, error, right, correct, category)\n",
    "        else:\n",
    "            report.fp(left, error, right, correct, category)\n",
    "    \n",
    "    report.lap(C)\n",
    "\n",
    "\n",
    "template = 'logs/test-%s-%d'\n",
    "report(get_name(template))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'A', u'DT', u'B-NP', u'O', u'NP-SBJ-1', u'a'], [u'bus', u'NN', u'I-NP', u'O', u'NP-SBJ-1', u'bus'], [u'looms', u'VBZ', u'B-VP', u'O', u'VP-1', u'loom'], [u'out', u'IN', u'B-PP', u'B-PNP', u'O', u'out'], [u'of', u'IN', u'I-PP', u'I-PNP', u'O', u'of'], [u'the', u'DT', u'B-NP', u'I-PNP', u'O', u'the'], [u'grey', u'JJ', u'I-NP', u'I-PNP', u'O', u'grey'], [u'and', u'CC', u'I-NP', u'I-PNP', u'O', u'and'], [u'blizzardy', u'JJ', u'I-NP', u'I-PNP', u'O', u'blizzardy'], [u'conditions', u'NNS', u'I-NP', u'I-PNP', u'O', u'condition'], [u'and', u'CC', u'O', u'O', u'O', u'and'], [u'we', u'PRP', u'B-NP', u'O', u'NP-SBJ-2', u'we'], [u'get', u'VB', u'B-VP', u'O', u'VP-2', u'get'], [u'on', u'IN', u'B-PP', u'O', u'O', u'on'], [u'gratefully', u'RB', u'B-ADVP', u'O', u'O', u'gratefully'], [u'.', u'.', u'O', u'O', u'O', u'.']] $ \n",
      "\n",
      "(u'A', u'DT', u'A')\n",
      "(u'bus', u'NN', u'bus')\n",
      "(u'looms', u'VBZ', u'looms')\n",
      "(u'out', u'IN', u'out')\n",
      "(u'of', u'IN', u'of')\n",
      "(u'the', u'DT', u'the')\n",
      "(u'grey', u'JJ', u'grey')\n",
      "(u'and', u'CC', u'and')\n",
      "(u'blizzardy', u'JJ', u'blizzardy')\n",
      "(u'conditions', u'NNS', u'conditions')\n",
      "(u'and', u'CC', u'and')\n",
      "(u'we', u'PRP', u'we')\n",
      "(u'get', u'VB', u'get')\n",
      "(u'on', u'IN', u'on')\n",
      "(u'gratefully', u'RB', u'gratefully')\n",
      "(u'.', u'.', u'.')\n",
      "[(u'A', u'DT', u'A'), (u'bus', u'NN', u'bus'), (u'looms', u'VBZ', u'looms'), (u'out', u'IN', u'out'), (u'of', u'IN', u'of'), (u'the', u'DT', u'the'), (u'grey', u'JJ', u'grey'), (u'and', u'CC', u'and'), (u'blizzardy', u'JJ', u'blizzardy'), (u'conditions', u'NNS', u'conditions'), (u'and', u'CC', u'and'), (u'we', u'PRP', u'we'), (u'get', u'VB', u'get'), (u'on', u'IN', u'on'), (u'gratefully', u'RB', u'gratefully'), (u'.', u'.', u'.')]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
