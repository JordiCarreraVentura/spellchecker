{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from config import CONFIG\n",
    "\n",
    "from tests import tests1, tests2\n",
    "\n",
    "from normalizer import Normalizer\n",
    "\n",
    "from lib.CharacterIndex import CharacterIndex\n",
    "from lib.NaiveTokenizer import NaiveTokenizer\n",
    "from lib.TextStreamer import TextStreamer\n",
    "from lib.CONLL14ErrorCorrection import CONLL14ErrorCorrection\n",
    "from lib.Parser import PatternParser\n",
    "from lib.Report import Report\n",
    "from lib.DistributionalModel import NgramModel\n",
    "\n",
    "from lib.Tools import (\n",
    "    FreqDist,\n",
    "    splitter,\n",
    "    strip_punct,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict as deft\n",
    ")\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    return '.'.join([str(t) for t in time.localtime()[3:6]])\n",
    "\n",
    "\n",
    "def get_name(template):\n",
    "    i = 1\n",
    "    while True:\n",
    "        name = template % (timestamp(), i)\n",
    "        if not os.path.exists(name):\n",
    "            return name\n",
    "        i += 1\n",
    "\n",
    "PoS_l = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT',\n",
    "         'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n",
    "         'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "PoS = {}\n",
    "i = 1\n",
    "for k in PoS_l:\n",
    "\tPoS[k] = i\n",
    "\ti += 1\n",
    "\n",
    "\n",
    "WORD_GRAMS = [\n",
    "    (1, False),\n",
    "    (2, False),\n",
    "    (3, False),\n",
    "#     (3, True),\n",
    "#     (4, True)\n",
    "]\n",
    "\n",
    "POS_GRAMS = [\n",
    "    (1, False),\n",
    "    (2, False),\n",
    "    (3, False),\n",
    "#     (3, True),\n",
    "#     (4, True)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "corpus1 = 'data/delorme.com_shu.pages_89.txt'\n",
    "corpus2 = 'data/delorme.com_shu.pages_102.txt'\n",
    "corpus3 = 'data/delorme.com_shu.pages_120.txt'\n",
    "corpus4 = 'data/utexas_iit.pages_12.txt'\n",
    "\n",
    "report = Report()\n",
    "\n",
    "parser = PatternParser()\n",
    "\n",
    "model = NgramModel(WORD_GRAMS)\n",
    "model_pos = NgramModel(POS_GRAMS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'A', u'DT', u'B-NP', u'O', u'NP-SBJ-1', u'a']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1bad39dce9ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[1;32mprint\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mraw_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mdump\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RudnikC\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         )\n\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RudnikC\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for C in CONFIG:\n",
    "    \n",
    "#     tests = tests1.items() + tests2.items()\n",
    "\n",
    "    conll = CONLL14ErrorCorrection()\n",
    "    \n",
    "    tests = []\n",
    "    for (left, err, right, corr, category), human  in conll:\n",
    "        if err:\n",
    "            test = (left, strip_punct(err).lower(), right,\n",
    "                    strip_punct(corr).lower(), category, True)\n",
    "        else:\n",
    "            test = (left, strip_punct(corr), right, err, category, False)\n",
    "        tests.append(test)\n",
    "    tests = tests[:30000]\n",
    "\n",
    "    targets = [test[1] for test in tests]\n",
    "\n",
    "\n",
    "    #\tCollect input from large text file:\n",
    "    dump = []\n",
    "#     for doc in TextStreamer(corpus, nb_sent=C['nb_sent']):\n",
    "    streamers = [\n",
    "        TextStreamer(corpus1, nb_sent=200000),\n",
    "#         TextStreamer(corpus2, nb_sent=200000),\n",
    "#         TextStreamer(corpus3, nb_sent=200000),\n",
    "#         TextStreamer(corpus4, nb_sent=200000),\n",
    "    ]\n",
    "    for streamer in streamers:\n",
    "        for doc in streamer:\n",
    "            for sent in splitter(doc):\n",
    "                parse = parser(sent)\n",
    "                # for unit in parse.split():\n",
    "                #    print unit[0]\n",
    "                # raw_input()\n",
    "                tokenized = [w.lower() for w in tokenizer(sent)]\n",
    "                tok_pos = [pos[1] for pos in parse.split()]\n",
    "                dump += tokenized\n",
    "                \n",
    "                model.update(['#'] + tokenized + ['#'])\n",
    "                model_pos.update(['#'] + tok_pos + ['#'])\n",
    "                \n",
    "    freq_dist = Counter(dump + targets)\n",
    "\n",
    "\n",
    "    #\tMap all character n-grams to words, and all words to their\n",
    "    #\tcharacter n-grams\n",
    "#     index = CharacterIndex(dump + targets, top_n=C['top_n'], min_r=C['sim_thres'])\n",
    "    index = CharacterIndex(dump + targets, top_n=C['top_n'], min_r=0.9)\n",
    "    index.build()\n",
    "\n",
    "    tests = [t for t in tests]\n",
    "    for i, (left, candidate, right, correct, category, is_candidate) in enumerate(tests):\n",
    "        \n",
    "        if candidate == correct:\n",
    "            continue\n",
    "#         elif is_candidate and (category != 'Mec'):\n",
    "#             continue\n",
    "\n",
    "        report.add()\n",
    "\n",
    "        if is_candidate and ((not correct or len(correct.split()) > 1)\n",
    "        or category not in ['Mec']):\n",
    "#         or category not in ['Mec', 'Nn', 'Wform']):\n",
    "            report.fn(left, candidate, right, correct, category)\n",
    "            continue\n",
    "\n",
    "#         similars = index(candidate, n=5)\n",
    "        similars = [(w, sim) for w, sim in index(candidate)\n",
    "                    if freq_dist[w] >= 10 and\n",
    "                    freq_dist[w] / freq_dist[candidate] >= 100]\n",
    "\n",
    "        if not similars and not is_candidate:\n",
    "            report.tn(left, candidate, right, correct, category)\n",
    "            continue\n",
    "        elif not similars:\n",
    "            report.fn(left, candidate, right, correct, category)\n",
    "            continue\n",
    "        elif similars and not is_candidate:\n",
    "            report.fp(left, candidate, right, correct, category)\n",
    "            continue\n",
    "\n",
    "#         similars.sort(\n",
    "#             key=lambda x: freq_dist[x[0]],\n",
    "#             reverse=True\n",
    "#         )\n",
    "#         top = [w for w, sim in similars[:1]]\n",
    "\n",
    "        corrections = []\n",
    "        for sim, _ in similars + [(candidate, None)]:\n",
    "            left = [e for _, e, _, _, _, _ in tests[i - 3:i]] + [sim]\n",
    "            right = [sim] + [e for _, e, _, _, _, _ in tests[i + 1:i + 4]]\n",
    "            \n",
    "            pos_context_left = ' '.join([e for _, e, _, _, _, _ in tests[i - 3:i]] \n",
    "                                   + [sim])\n",
    "            pos_context_right = ' '.join([sim]\n",
    "                                   + [e for _, e, _, _, _, _ in tests[i + 1:i + 4]])\n",
    "            \n",
    "            parse_pos_left = parser(pos_context_left)\n",
    "            parse_pos_right = parser(pos_context_right)\n",
    "            \n",
    "            left_pos = [e_pos[1] for e_pos in parse_pos_left]\n",
    "            right_pos = [e_pos[1] for e_pos in parse_pos_right]\n",
    "            \n",
    "            pleft = model(left)\n",
    "            pright = model(right)\n",
    "            \n",
    "            pleft_pos = model_pos(left_pos)\n",
    "            pright_pos = model_pos(right_pos)\n",
    "            \n",
    "            score = abs(pleft - pright)\n",
    "            \n",
    "            print left, pleft\n",
    "            print right, pright\n",
    "\t\t\t\n",
    "#             corrections.append((score, sim))\n",
    "            corrections.append((score * max([pleft, pright]), sim))\n",
    "        baseline = [sim for sim, w in corrections if w == candidate][0]\n",
    "\n",
    "#         print [(freq_dist[w] / freq_dist[candidate], w) for sim, w in corrections[:1]\n",
    "#                    if freq_dist[w] / freq_dist[candidate] >= 2]\n",
    "#         print [w for sim, w in corrections[:1]\n",
    "#                    if (baseline and sim / baseline >= 2)\n",
    "#                    or not baseline]\n",
    "#         print\n",
    "\n",
    "        if baseline:\n",
    "            top = [w for sim, w in corrections[:1]\n",
    "                   if w != candidate and\n",
    "                   freq_dist[w] / freq_dist[candidate] >= 300]\n",
    "#             print [(w, sim / baseline) for sim, w in corrections[:1]\n",
    "#                    if w != candidate and\n",
    "#                    freq_dist[w] / freq_dist[candidate] >= 10 and\n",
    "#                    sim / baseline >= 1000]\n",
    "        else:\n",
    "            top = [w for sim, w in corrections[:1]]\n",
    "\n",
    "        if not top and is_candidate:\n",
    "            report.fn(left, candidate, right, correct, category)\n",
    "        elif not top and not is_candidate:\n",
    "            report.tn(left, candidate, right, correct, category)\n",
    "        elif is_candidate and correct in top:\n",
    "            report.tp(left, candidate, right, correct, category)\n",
    "        elif top and not is_candidate:\n",
    "            report.fp(left, candidate, right, correct, category)\n",
    "    \n",
    "    report.lap(C)\n",
    "    break\n",
    "\n",
    "\n",
    "template = 'logs/test-%s-%d'\n",
    "report(get_name(template))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
