{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from config import CONFIG\n",
    "\n",
    "from tests import tests1, tests2\n",
    "\n",
    "from normalizer import Normalizer\n",
    "\n",
    "from lib.CharacterIndex import CharacterIndex\n",
    "from lib.NaiveTokenizer import NaiveTokenizer\n",
    "from lib.TextStreamer import TextStreamer\n",
    "from lib.CONLL14ErrorCorrection import CONLL14ErrorCorrection\n",
    "from lib.Parser import PatternParser\n",
    "from lib.Report import Report\n",
    "from lib.DistributionalModel import NgramModel\n",
    "\n",
    "from lib.Tools import (\n",
    "    FreqDist,\n",
    "    splitter,\n",
    "    strip_punct,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict as deft\n",
    ")\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    return '.'.join([str(t) for t in time.localtime()[3:6]])\n",
    "\n",
    "\n",
    "def get_name(template):\n",
    "    i = 1\n",
    "    while True:\n",
    "        name = template % (timestamp(), i)\n",
    "        if not os.path.exists(name):\n",
    "            return name\n",
    "        i += 1\n",
    "\n",
    "PoS_l = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT',\n",
    "         'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n",
    "         'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "PoS = {}\n",
    "i = 1\n",
    "for k in PoS_l:\n",
    "\tPoS[k] = i\n",
    "\ti += 1\n",
    "\n",
    "\n",
    "WORD_GRAMS = [\n",
    "    (1, False),\n",
    "    (2, False),\n",
    "    (3, False),\n",
    "#     (3, True),\n",
    "#     (4, True)\n",
    "]\n",
    "\n",
    "POS_GRAMS = [\n",
    "    (1, False),\n",
    "    (2, False),\n",
    "    (3, False),\n",
    "#     (3, True),\n",
    "#     (4, True)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "corpus1 = 'data/delorme.com_shu.pages_89.txt'\n",
    "corpus2 = 'data/delorme.com_shu.pages_102.txt'\n",
    "corpus3 = 'data/delorme.com_shu.pages_120.txt'\n",
    "corpus4 = 'data/utexas_iit.pages_12.txt'\n",
    "\n",
    "report = Report()\n",
    "\n",
    "parser = PatternParser()\n",
    "\n",
    "model = NgramModel(WORD_GRAMS)\n",
    "model_pos = NgramModel(POS_GRAMS)\n",
    "\n",
    "C = CONFIG[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1990it [00:14, 193.01it/s]\n"
     ]
    }
   ],
   "source": [
    " #for C in CONFIG:\n",
    "    \n",
    "#     tests = tests1.items() + tests2.items()\n",
    "\n",
    "conll = CONLL14ErrorCorrection()\n",
    "\n",
    "tests = []\n",
    "for (left, err, right, corr, category), human  in conll:\n",
    "    if err:\n",
    "        test = (left, strip_punct(err).lower(), right,\n",
    "                strip_punct(corr).lower(), category, True)\n",
    "    else:\n",
    "        test = (left, strip_punct(corr), right, err, category, False)\n",
    "    tests.append(test)\n",
    "tests = tests[:30000]\n",
    "\n",
    "targets = [test[1] for test in tests]\n",
    "\n",
    "\n",
    "#\tCollect input from large text file:\n",
    "dump = []\n",
    "#     for doc in TextStreamer(corpus, nb_sent=C['nb_sent']):\n",
    "streamers = [\n",
    "    TextStreamer(corpus1, nb_sent=2000),\n",
    "#         TextStreamer(corpus2, nb_sent=200000),\n",
    "#         TextStreamer(corpus3, nb_sent=200000),\n",
    "#         TextStreamer(corpus4, nb_sent=200000),\n",
    "]\n",
    "for streamer in streamers:\n",
    "    for doc in streamer:\n",
    "        for sent in splitter(doc):\n",
    "            parse = parser(sent)\n",
    "            # for unit in parse.split():\n",
    "            #    print unit[0]\n",
    "            # raw_input()\n",
    "            tokenized = [w.lower() for w in tokenizer(sent)]\n",
    "            tok_pos = [pos[1] for pos in parse.split()[0]]\n",
    "            dump += tokenized\n",
    "\n",
    "#                print tokenized\n",
    "#            print 'parse' , parse.split()\n",
    "#            print 'parse' , parse.split()[0]\n",
    "#            print 'tok pos', tok_pos\n",
    "#            raw_input()\n",
    "\n",
    "            model.update(['#'] + tokenized + ['#'])\n",
    "            model_pos.update(['#'] + tok_pos + ['#'])\n",
    "\n",
    "freq_dist = Counter(dump + targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    #\tMap all character n-grams to words, and all words to their\n",
    "    #\tcharacter n-grams\n",
    "    #\tindex = CharacterIndex(dump + targets, top_n=C['top_n'], min_r=C['sim_thres'])\n",
    "    index = CharacterIndex(dump + targets, top_n=C['top_n'], min_r=0.9)\n",
    "    index.build()\n",
    "\n",
    "    tests = [t for t in tests]\n",
    "    for i, (left, candidate, right, correct, category, is_candidate) in enumerate(tests):\n",
    "        \n",
    "        if candidate == correct:\n",
    "            continue\n",
    "#         elif is_candidate and (category != 'Mec'):\n",
    "#             continue\n",
    "\n",
    "        report.add()\n",
    "        if is_candidate and ((not correct or len(correct.split()) > 1)\n",
    "        or category not in ['Mec']):\n",
    "#         or category not in ['Mec', 'Nn', 'Wform']):\n",
    "            report.fn(left, candidate, right, correct, category)\n",
    "            continue\n",
    "\n",
    "#         similars = index(candidate, n=5)\n",
    "        similars = [(w, sim) for w, sim in index(candidate)\n",
    "                    if freq_dist[w] >= 10 and\n",
    "                    freq_dist[w] / freq_dist[candidate] >= 100]\n",
    "\n",
    "        if not similars and not is_candidate:\n",
    "            report.tn(left, candidate, right, correct, category)\n",
    "            continue\n",
    "        elif not similars:\n",
    "            report.fn(left, candidate, right, correct, category)\n",
    "            continue\n",
    "        elif similars and not is_candidate:\n",
    "            report.fp(left, candidate, right, correct, category)\n",
    "            continue\n",
    "\n",
    "#         similars.sort(\n",
    "#             key=lambda x: freq_dist[x[0]],\n",
    "#             reverse=True\n",
    "#         )\n",
    "#         top = [w for w, sim in similars[:1]]\n",
    "\n",
    "        corrections = []\n",
    "        for sim, _ in similars + [(candidate, None)]:\n",
    "            left = [e for _, e, _, _, _, _ in tests[i - 3:i]] + [sim]\n",
    "            right = [sim] + [e for _, e, _, _, _, _ in tests[i + 1:i + 4]]\n",
    "            \n",
    "            pos_context_left = ' '.join([e for _, e, _, _, _, _ in tests[i - 3:i]] \n",
    "                                   + [sim])\n",
    "            pos_context_right = ' '.join([sim]\n",
    "                                   + [e for _, e, _, _, _, _ in tests[i + 1:i + 4]])\n",
    "            \n",
    "            parse_pos_left = parser(pos_context_left)\n",
    "            parse_pos_right = parser(pos_context_right)\n",
    "            \n",
    "#            print 'pos_context_left', pos_context_left, '\\n'\n",
    "#            print 'parse_pos_left', parse_pos_left, '\\n'\n",
    "#            print 'pos_context_left.split()', pos_context_left.split()\n",
    "#            raw_input()\n",
    "            \n",
    "            left_pos = [e_pos[1] for e_pos in parse_pos_left.split()[0]]\n",
    "            right_pos = [e_pos[1] for e_pos in parse_pos_right.split()[0]]\n",
    "            \n",
    "            pleft = model(left)\n",
    "            pright = model(right)\n",
    "            \n",
    "            pleft_pos = model_pos(left_pos)\n",
    "            pright_pos = model_pos(right_pos)\n",
    "            \n",
    "            score = abs(pleft - pright)\n",
    "            score_pos = abs(pleft_pos - pright_pos)\n",
    "            \n",
    "            print left, pleft\n",
    "            print left_pos, pleft_pos\n",
    "            print right, pright\n",
    "            print right_pos, pright_pos\n",
    "            \n",
    "            corrections.append((score * max([pleft, pright]), (score_pos * max([pleft_pos, pright_pos]) , sim))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corrections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4e06d744116b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#           corrections.append((score, sim))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbaseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msim_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_pos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msim_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorrections\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msim_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorrections\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#         print [(freq_dist[w] / freq_dist[candidate], w) for sim, w in corrections[:1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corrections' is not defined"
     ]
    }
   ],
   "source": [
    "\t\t\t\n",
    "#           corrections.append((score, sim))\n",
    "            \n",
    "baseline = [[sim_w, sim_pos] for sim_w, sim_pos, w in corrections if w == candidate][0]\n",
    "top = [w for sim_w, sim_pos, w in corrections if w == candidate][0]\n",
    "#         print [(freq_dist[w] / freq_dist[candidate], w) for sim, w in corrections[:1]\n",
    "#                    if freq_dist[w] / freq_dist[candidate] >= 2]\n",
    "#         print [w for sim, w in corrections[:1]\n",
    "#                    if (baseline and sim / baseline >= 2)\n",
    "#                    or not baseline]\n",
    "#         print\n",
    "\n",
    "print corrections\n",
    "\n",
    "#hypothesis = [sim_w, sim_pos, w in corrections[:1] if w != candidate]\n",
    "#        top_w = [w for sim_w, sim_pos, w in corrections[:1] if w != candidate]\n",
    "#        top_sim_w = [sim_w for sim_w, sim_pos, w in corrections[:1] if w != candidate]\n",
    "#        top_sim_pos = [sim_pos for sim_w, sim_pos, w in corrections[:1] if w != candidate]\n",
    "\n",
    "for hyp in hypothesis:\n",
    "    if hyp[0] > baseline[0] and hyp[1] > baseline[1]:\n",
    "        baseline = [hyp[0],hyp[1]]\n",
    "        top = hyp[3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#        if baseline :\n",
    "#            top = [w for sim_w, sim_pos, w in corrections[:1]\n",
    "#                   if w != candidate and\n",
    "#                   freq_dist[w] / freq_dist[candidate] >= 300]\n",
    "#             print [(w, sim / baseline) for sim, w in corrections[:1]\n",
    "#                    if w != candidate and\n",
    "#                    freq_dist[w] / freq_dist[candidate] >= 10 and\n",
    "#                    sim / baseline >= 1000]\n",
    "#        else:\n",
    "#            top = [w for sim_w, sim_pos, w in corrections[:1]]\n",
    "\n",
    "if not top and is_candidate:\n",
    "    report.fn(left, candidate, right, correct, category)\n",
    "elif not top and not is_candidate:\n",
    "    report.tn(left, candidate, right, correct, category)\n",
    "elif is_candidate and correct in top:\n",
    "    report.tp(left, candidate, right, correct, category)\n",
    "elif top and not is_candidate:\n",
    "    report.fp(left, candidate, right, correct, category)\n",
    "\n",
    "# report.lap(C)\n",
    "# break\n",
    "\n",
    "\n",
    "#template = 'logs/test-%s-%d'\n",
    "#report(get_name(template))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq_ratio': 10,\n",
       " 'min_Count': 10,\n",
       " 'nb_sent': 50000,\n",
       " 'sim_thres': 0.85,\n",
       " 'top_n': 50}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
